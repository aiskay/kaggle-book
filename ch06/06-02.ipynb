{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 6.2 特徴選択および特徴量の重要度\n",
    "\n",
    "モデルの予測に寄与しない特徴を判定することで、それらを取り除き精度向上や計算時間の短縮が可能となる。\n",
    "\n",
    "ここでは、以下の 3 つの方法を用いてそのような特徴量の判定方法を説明する。\n",
    "\n",
    "1. 単変量統計を用いる方法\n",
    "2. 特徴量の重要度を用いる方法\n",
    "3. 反復して探索する方法"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# ---------------------------------\n",
    "# データ等の準備\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# train_x は学習データ、train_y は目的変数、test_x はテストデータ\n",
    "train = pd.read_csv('../input/sample-data/train_preprocessed_onehot.csv')\n",
    "train_x = train.drop(['target'], axis=1)\n",
    "train_y = train['target']\n",
    "test_x = pd.read_csv('../input/sample-data/test_preprocessed_onehot.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2.1 単変量統計を用いる方法\n",
    "\n",
    "各特徴量と目的変数から何らかの統計量を計算し、その統計量の順序で特徴量を選択することを考える。\n",
    "\n",
    "その中で、特徴量と目的変数の 1:1 の関係に着目した単変量統計について考える。\n",
    "\n",
    "### 相関係数\n",
    "\n",
    "各特徴量 $x_i$ と目的変数 $y_i$ の相関係数 (ピアソンの積率相関係数)\n",
    "\n",
    "$$\n",
    "\\rho = \\frac{\\sum_i(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_i(x_i - {x})^2\\sum_i(y_i - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "を計算してその絶対値の大きい順に選択する。  \n",
    "線形以外の関係性を捉えることはできないので注意。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# ---------------------------------\n",
    "# 相関係数\n",
    "# ---------------------------------\n",
    "import scipy.stats as st\n",
    "\n",
    "# 相関係数\n",
    "corrs = []\n",
    "for c in train_x.columns:\n",
    "    corr = np.corrcoef(train_x[c], train_y)[0, 1]\n",
    "    corrs.append(corr)\n",
    "corrs = np.array(corrs)\n",
    "\n",
    "# 重要度の上位を出力する（上位5個まで）\n",
    "# np.argsortを使うことで、値の順序のとおりに並べたインデックスを取得できる\n",
    "idx = np.argsort(np.abs(corrs))[::-1]\n",
    "top_cols, top_importances = train_x.columns.values[idx][:5], corrs[idx][:5]\n",
    "print(top_cols, top_importances)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['medical_info_a1' 'medical_keyword_5' 'medical_keyword_4'\n",
      " 'medical_keyword_3' 'age'] [0.21805214 0.21368557 0.18109642 0.16723961 0.15155308]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "スピアマンの順位相関係数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# スピアマンの順位相関係数\n",
    "corrs_sp = []\n",
    "for c in train_x.columns:\n",
    "    corr_sp = st.spearmanr(train_x[c], train_y).correlation\n",
    "    corrs_sp.append(corr_sp)\n",
    "corrs_sp = np.array(corrs_sp)\n",
    "\n",
    "idx2 = np.argsort(np.abs(corrs_sp))[::-1]\n",
    "top_cols2, top_importances2 = train_x.columns.values[idx][:5], corrs_sp[idx][:5]\n",
    "print(top_cols2, top_importances2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['medical_info_a1' 'medical_keyword_5' 'medical_keyword_4'\n",
      " 'medical_keyword_3' 'age'] [0.22182331 0.21368557 0.18109642 0.16723961 0.15170291]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### カイ二乗統計量\n",
    "\n",
    "各特徴量と目的変数について独立性の検定を行い、カイ二乗検定の統計量を計算する。  \n",
    "\n",
    "$i$ 番目の特徴量について、target が $j$ である $k (k=1, 2, ..., n)$ 番目のデータを $f_{ijk}$ として、観測度数 $O$ と期待度数 $E$ を定義する。\n",
    "\n",
    "$$\n",
    "O_{ij} = \\sum_{k}^n f_{ijk}, \\quad E_{ij} = \\frac{({\\rm number\\ of\\ data\\ with\\ target}\\ j)}{n}.\n",
    "$$\n",
    "\n",
    "もし $i$ 番目の特徴量と target が独立であれば、以下の統計量は自由度 $n-1$ の $\\chi^2$ 分布に従うはずである。\n",
    "\n",
    "$$\n",
    "\\chi^2_i \\equiv \\sum_j \\frac{(O_{ij} - E_{ij})^2}{E_{ij}} \\sim \\chi^2(n-1).\n",
    "$$\n",
    "\n",
    "従って、以上の統計量を求めることで、独立かどうかの判定を行うことができる。\n",
    "\n",
    "- 特徴量の値でスケールされるので、min-max scaling などを事前に行う\n",
    "- 独立性の検定は一般に頻度を表す特徴量について用いられるが、機械学習の文脈だと連続値に対しても応用されている  \n",
    "  従って、分類タスクでの非負の特徴量についてのみ使える\n",
    "\n",
    "python では `sklearn.feature_selection` を使えば簡単に実装できる。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# カイ二乗統計量\n",
    "x = MinMaxScaler().fit_transform(train_x)\n",
    "c2, _ = chi2(x, train_y)\n",
    "\n",
    "# 重要度の上位を出力する（上位5個まで）\n",
    "idx = np.argsort(c2)[::-1]\n",
    "top_cols, top_importances = train_x.columns.values[idx][:5], corrs[idx][:5]\n",
    "print(top_cols, top_importances)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 相互情報量\n",
    "\n",
    "各特徴量と目的変数の相互情報量\n",
    "\n",
    "$$\n",
    "I(X; Y) = \\int_X \\int_Y p(x, y) \\log \\frac{p(x,y)}{p(x)p(y)} dxdy\n",
    "$$\n",
    "\n",
    "を計算し、大きいものから特徴量を選択する。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---------------------------------\n",
    "# 相互情報量\n",
    "# ---------------------------------\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# 相互情報量\n",
    "mi = mutual_info_classif(train_x, train_y)\n",
    "\n",
    "# 重要度の上位を出力する（上位5個まで）\n",
    "idx = np.argsort(mi)[::-1]\n",
    "top_cols, top_importances = train_x.columns.values[idx][:5], corrs[idx][:5]\n",
    "print(top_cols, top_importances)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "学習データ全体で特徴量選択をしてしまうと、本来は目的変数と関係ないにもかかわらずたまたま学習データで偏りが出ている特徴量が有効な特徴量と認識されてしまうことがある。  \n",
    "そのため、特徴量選択も out-of-fold で行ったほうがよいいこともあると覚えておくとよい。\n",
    "\n",
    "## 6.2.2 特徴量の重要度を用いる方法\n",
    "\n",
    "モデルから出力される特徴量の重要度を用いて特徴量を選択する方法\n",
    "\n",
    "### ランダムフォレストの特徴量の重要度\n",
    "\n",
    "### GBDT の特徴量の重要度\n",
    "\n",
    "### その他の手法\n",
    "\n",
    "#### permutation importance\n",
    "\n",
    "モデルを学習した後、validation data のある特徴量をシャッフルした場合の予測とシャッフルしていない予測を比較し、予測精度の落ち具合から特徴量の重要度を判定する方法。  \n",
    "`eli5` というライブラリが使える他、random forest の場合、学習データのサンプリングから外れた out-of-bag と呼ばれるデータを用いて `rfpimp` などのモジュールにより計算することができる。\n",
    "\n",
    "#### null importance\n",
    "\n",
    "目的変数をシャッフルして学習させた場合のモデルの重要度を null importance として基準とし、目的変数をシャッフルさせていない通常の重要度と比較して特徴量を選択する手法。  \n",
    "null importance はシャッフルによって変わるため、数十回繰り返して統計量を用いる。\n",
    "\n",
    "実装例は Home Credit Default Risk の kaggle kenel [Feature Selection with Null Importances](https://www.kaggle.com/ogrellier/feature-selection-with-null-importances) を参考。\n",
    "\n",
    "#### boruta\n",
    "\n",
    "それぞれの特徴量をシャッフルしたデータ shadow feature を元のデータに加えて random forest で学習を行い、それぞれの特徴量の重要度が全ての shadow feature より大きいものを記録する。  \n",
    "これを何度か繰り返し、shadow feature より重要とは言えない特徴量を除外していく。\n",
    "\n",
    "ライブラリ [`BorutaPy`](https://danielhomola.com/boruta_py) が公開されており、実装例は kaggle kernel [Boruta feature elimination](https://www.kaggle.com/tilii7/boruta-feature-elimination)を参考のこと。\n",
    "\n",
    "#### 特徴量を大量生成してからの特徴量選択\n",
    "\n",
    "機械的に特徴量を大量生成してから特徴量選択をする手法。\n",
    "\n",
    "#### xgbfir\n",
    "\n",
    "xgboost のモデルから決定木分岐の情報を抽出して特徴量の重要度を出力するライブラリ。  \n",
    "2017 以降更新されてなさそうなので、使うことはなさそう...？\n",
    "\n",
    "## 6.2.3 反復して探索する方法\n",
    "\n",
    "特徴量の組み合わせを変えて学習を繰り返し、その精度などを用いて探索する手法。  \n",
    "時間もかかるしあまりやることはないような気がするので省略。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}